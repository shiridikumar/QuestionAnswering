{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import argparse\n",
    "import collections\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "from io import open\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Squad-Dataset Example and Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whitespace_tokenize(text):\n",
    "    \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    tokens = text.split()\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquadObject(object):\n",
    "    def __init__(self, q_id, ques_txt, doc_tokens, original_ans=None, start_position=None, end_position=None, is_impossible=None):\n",
    "        self.q_id = q_id\n",
    "        self.ques_txt = ques_txt\n",
    "        self.doc_tokens = doc_tokens\n",
    "        self.original_ans = original_ans\n",
    "        self.start_position = start_position\n",
    "        self.end_position = end_position\n",
    "        self.is_impossible = is_impossible\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        s = \"\"\n",
    "        s += \"Question ID: %s\" % (self.q_id)\n",
    "        s += \", Question Text: %s\" % (self.ques_txt)\n",
    "        s += \", Document Tokens: %s\" % (self.doc_tokens)\n",
    "        if self.original_ans:\n",
    "            s += \", Original Answer: %s\" % (self.original_ans)\n",
    "        if self.start_position:\n",
    "            s += \", Start Position: %d\" % (self.start_position)\n",
    "        if self.end_position:\n",
    "            s += \", End Position: %d\" % (self.end_position)\n",
    "        if self.is_impossible:\n",
    "            s += \", Is Impossible: %s\" % (self.is_impossible)\n",
    "        return s\n",
    "    \n",
    "def whitepace_tokenize(text):\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def read_squad_examples(input_file, version_2_with_negative,method='training'):\n",
    "    \"\"\"Read a SQuAD json file into a list of SquadExample.\"\"\"\n",
    "    with open(input_file, \"r\", encoding='utf-8') as reader:\n",
    "        input_data = json.load(reader)[\"data\"]\n",
    "\n",
    "    def is_whitespace(c):\n",
    "        if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    examples = []\n",
    "    for entry in input_data:\n",
    "        for paragraph in entry[\"paragraphs\"]:\n",
    "            paragraph_text = paragraph[\"context\"]\n",
    "            doc_tokens = []\n",
    "            char_to_word_offset = []\n",
    "            prev_is_whitespace = True\n",
    "            for c in paragraph_text:\n",
    "                if is_whitespace(c):\n",
    "                    prev_is_whitespace = True\n",
    "                else:\n",
    "                    if prev_is_whitespace:\n",
    "                        doc_tokens.append(c)\n",
    "                    else:\n",
    "                        doc_tokens[-1] += c\n",
    "                    prev_is_whitespace = False\n",
    "                char_to_word_offset.append(len(doc_tokens) - 1)\n",
    "\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                qas_id = qa[\"id\"]\n",
    "                question_text = qa[\"question\"]\n",
    "                start_position = None\n",
    "                end_position = None\n",
    "                orig_answer_text = None\n",
    "                is_impossible = False\n",
    "                if method == 'training':\n",
    "                    if version_2_with_negative:\n",
    "                        is_impossible = qa[\"is_impossible\"]\n",
    "                    if (len(qa[\"answers\"]) != 1) and (not is_impossible):\n",
    "                        raise ValueError(\n",
    "                            \"For training, each question should have exactly 1 answer.\")\n",
    "                    if not is_impossible:\n",
    "                        answer = qa[\"answers\"][0]\n",
    "                        orig_answer_text = answer[\"text\"]\n",
    "                        answer_offset = answer[\"answer_start\"]\n",
    "                        answer_length = len(orig_answer_text)\n",
    "                        start_position = char_to_word_offset[answer_offset]\n",
    "                        end_position = char_to_word_offset[answer_offset +\n",
    "                                                           answer_length - 1]\n",
    "                        actual_text = \" \".join(\n",
    "                            doc_tokens[start_position:(end_position + 1)])\n",
    "                        cleaned_answer_text = \" \".join(\n",
    "                            whitespace_tokenize(orig_answer_text))\n",
    "                        if actual_text.find(cleaned_answer_text) == -1:\n",
    "                            logger.warning(\"Could not find answer: '%s' vs. '%s'\",\n",
    "                                           actual_text, cleaned_answer_text)\n",
    "                            continue\n",
    "                    else:\n",
    "                        start_position = -1\n",
    "                        end_position = -1\n",
    "                        orig_answer_text = \"\"\n",
    "\n",
    "                example = SquadObject(\n",
    "                    q_id=qas_id,\n",
    "                    ques_txt=question_text,\n",
    "                    doc_tokens=doc_tokens,\n",
    "                    original_ans=orig_answer_text,\n",
    "                    start_position=start_position,\n",
    "                    end_position=end_position,\n",
    "                    is_impossible=is_impossible)\n",
    "                \n",
    "                examples.append(example)\n",
    "    return examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training entries: 130319\n"
     ]
    }
   ],
   "source": [
    "training_file = '../Data/train-v2.0.json'\n",
    "version_2_negative = True\n",
    "training_entries =  read_squad_examples(\n",
    "    training_file,version_2_negative, )\n",
    "print(\"Number of training entries: %d\" % (len(training_entries)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question ID: 56be85543aeaaa14008c9063, Question Text: When did Beyonce start becoming popular?, Document Tokens: ['Beyoncé', 'Giselle', 'Knowles-Carter', '(/biːˈjɒnseɪ/', 'bee-YON-say)', '(born', 'September', '4,', '1981)', 'is', 'an', 'American', 'singer,', 'songwriter,', 'record', 'producer', 'and', 'actress.', 'Born', 'and', 'raised', 'in', 'Houston,', 'Texas,', 'she', 'performed', 'in', 'various', 'singing', 'and', 'dancing', 'competitions', 'as', 'a', 'child,', 'and', 'rose', 'to', 'fame', 'in', 'the', 'late', '1990s', 'as', 'lead', 'singer', 'of', 'R&B', 'girl-group', \"Destiny's\", 'Child.', 'Managed', 'by', 'her', 'father,', 'Mathew', 'Knowles,', 'the', 'group', 'became', 'one', 'of', 'the', \"world's\", 'best-selling', 'girl', 'groups', 'of', 'all', 'time.', 'Their', 'hiatus', 'saw', 'the', 'release', 'of', \"Beyoncé's\", 'debut', 'album,', 'Dangerously', 'in', 'Love', '(2003),', 'which', 'established', 'her', 'as', 'a', 'solo', 'artist', 'worldwide,', 'earned', 'five', 'Grammy', 'Awards', 'and', 'featured', 'the', 'Billboard', 'Hot', '100', 'number-one', 'singles', '\"Crazy', 'in', 'Love\"', 'and', '\"Baby', 'Boy\".'], Original Answer: in the late 1990s, Start Position: 39, End Position: 42\n"
     ]
    }
   ],
   "source": [
    "print(training_entries[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
